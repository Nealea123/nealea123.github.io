---
title: "Long-order operation tasks for skill reinforcement learning of residual hypernetworks"
date: 2025-05-05
layout: post
categories: [Hypernetworks, Reinforcement Learning]
tags: [Hypernetworks, Reinforcement Learning, Robust, Manipulation]
---

## Introduction
A reinforcement learning system based on residual hypernetwork pools and real-time human demonstrations is proposed to achieve robust learning for long-order and complex arm operation tasks. This system enables the robotic arm to continuously try and explore through reinforcement learning. It first trains the basic model for grasping, and then learns different branch skills through the hypernetwork pool. Under the real-time teaching of the operator, it acquires more refined operation skills and features high robustness and a 100% success rate.

## Flowchart
![hil hyper](/images/hil/hil_hyper.png)


## Experiment
<html>
<body>
<div style="text-align: center">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/-7dYe60Fb_8?si=EiibZphorwj1_BHR" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
  <!-- <p style="margin-top: 10px;">chigui-2 motion video in May, 2022</p> -->
</div>
</body>
</html>


<!-- The video shows a robot imitation learning experiment based on ROS/Gazebo. In the simulation environment, a robotic arm is operating a red and blue cube on the workbench. The "color_image" window in the top left corner displays the image information from the perspective of the robotic arm. This experiment aims to train the robot to complete cube manipulation tasks through imitation learning. -->

<!-- > A preprint of the paper is available at <kbd><a href="https://arxiv.org/abs/2404.12220" target="_blank" style="text-decoration: none; color: inherit;" >arXiv</a></kbd>
{: .prompt-tip } -->

<!-- , which is submitted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024 -->

<!-- ## Motivation -->
<!-- ![Motivation](/images/slednav/sledinspir.bmp) -->


<!-- The figure presents a training process based on action block imitation learning.  In Step 1, it samples data from a demo dataset, including RGB images and joints information, to form an action sequence.  Step 2 involves inferring z through a transformer encoder with self - attention blocks, embedding joints and action sequences.  In Step 3, it uses a ResNet18 and transformer encoder and decoder to predict the action sequence, incorporating position embeddings and linear layers to output the predicted actions. -->

<!-- ![Physhical Curves](/images/arm/physical_curves.png) -->

<!-- The figure illustrates a Transformer - based testing process for predicting action sequences. ResNet18 extracts features from images of four cameras. These features, combined with sinusoidal position embeddings, are fed into a Transformer encoder. The encoder processes them and passes to a Transformer decoder, which generates the predicted action sequence with the aid of position embeddings. -->


<!-- ![Experiment](/images/slednav/sledtest.bmp)
**Experiment result**: tractor average speed **<font color="red">≈1m/s</font>**, trailer position error **<font color="red"><10cm</font>**, and trailer yaw angle error **<font color="red"><5.25°</font>** (calculated by motion capture system) -->